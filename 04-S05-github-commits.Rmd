## Compare download counts with the number of commits on master branch

In this section, we compared last half a year's total downloads with the number of commits on master branch in Github repositories. And this turn, we initially planned to replace the research object with all the R-packages on CRAN, `r comma(length(pkgs$package),digits =0)` up to now. But we soon found out that there were only 6185 R-packages that have a Github repository, and after cleaning up, only 5769 remained.

```{block, type="discovery", echo = TRUE}
**Finding**: In our initial assumption, more commits on master branch of Github repository tends to bring more download counts to R-packages. On the one hand, the number of commits indicates that developers are constantly supplementing and updating the R-package, which will lead to more downloads. On the other hand, the number of commits also reflects the attention developers attach to the R-package to a certain extent. So, if an R-package has more commits, then developers may invest more in advertising and other ways to promote it, so as to expand the popularity and improve the download count.
```

The way we initially applied was extracting the commits through accessing the Github API. To achieve this, we first scraped the Github URLs from 'description' page for all the R-packages, and cleaned up the multiple URLs or some other redundant symbols and characters. And the URL of scraping the commits through Github API is formatted as *"https://api.github.com/repos/{user_repo}/commits?per_page=1"*. For example, the URL for package `tidyverse` can be like *"https://api.github.com/repos/tidyverse/tidyverse/commits?per_page=1"*. So, practically, by replacing the `user_repo` part that consists of the name of the related repository and the name of the package holder, we could access the content of all the package URLs through Github API.

Based on that, Table \@ref(tab:userrepo-tbl) shows the first 5 R-packages of the whole, their Github URLs and the `user_repo` part.

However, the Github API has a rate limits allowing for up to 60 requests per hour for unauthenticated requests, and this can be extended to 5000 per hour after authentication[@githubapi]. But even after getting authentication, our rate limit didn't get promoted. So, we switched to scrape commits by python spider. And by setting random user agent, we successfully avoided the API limit. And as this method is quite time consuming (around 5 hours), we saved the output as a .txt file then load it in R studio to speed up the compiling process. 

```{r userrepo,eval=FALSE}
#code used to extract Github URL for CRAN packages

#scrape the Github URLs
getPackageRDS <- function() {
     description <- sprintf("%s/web/packages/packages.rds",
                            getOption("repos")["CRAN"])
     con <- if(substring(description, 1L, 7L) == "file://") {
         file(description, "rb")
     } else {
         url(description, "rb")
     }
     on.exit(close(con))
     db <- readRDS(gzcon(con))
     rownames(db) <- NULL
     return(db)
}

dd <- as.data.frame(getPackageRDS())
dd2 <- subset(dd, grepl("github.com", URL))

## clean up (multiple URLs, etc.)
dd2$URL <- sapply(strsplit(dd2$URL,"[, \n]"),
       function(x) trimws(grep("github.com", x, value=TRUE)[1]))

dd2$URL <- sapply(strsplit(dd2$URL,"[>]"),
       function(x) trimws(grep("github.com", x, value=TRUE)[1]))

dd2$URL <- sapply(strsplit(dd2$URL,"[<]"),
       function(x) trimws(grep("github.com", x, value=TRUE)[1]))

dd2$URL <- sapply(strsplit(dd2$URL,"[#]"),
       function(x) trimws(grep("github.com", x, value=TRUE)[1]))

dd2$URL <- str_replace_all(dd2$URL, "https", "http")


dd2_new <- dd2 %>%
  select(Package,URL) %>%
  mutate(user_repo = str_sub(URL, str_locate(URL, 'github.com/')[2]+11)) 

#filter some obs
a <- dd2_new$user_repo[str_detect(dd2_new$user_repo,'/blob/')]
b <- dd2_new$user_repo[str_detect(dd2_new$user_repo,'/master')]
c <- dd2_new$user_repo[str_detect(dd2_new$user_repo,'/Master')]

#get all the username and repo name done
dd2_new <- dd2_new %>%
  filter(!(user_repo%in%a))%>%
  filter(!(user_repo%in%b))%>%
  filter(!(user_repo%in%c))

#want to remove the "/" at the last position of user_repo for some pkgs
#count the frequency of "/"
dd2_new$freq_dash <- str_count(dd2_new$user_repo, "/")

#remove the last "/" for those have two "/" in user_repo (and don't have other things inside)
dd2_new_dash <- dd2_new %>%
  filter(freq_dash == 2) %>%
  mutate(user_repo = substr(user_repo,1,nchar(user_repo)-1))
         
#bind with original data and filter some pkgs with unclean user_repo (hard to clean and not many)
dd2_new <- dd2_new %>%
  filter(!(Package %in% dd2_new_dash$Package)) %>%
  rbind(dd2_new_dash) %>%
  arrange(Package)%>%
  filter(!(freq_dash == 0))%>%
  filter(!(freq_dash == 2))%>%
  filter(!(freq_dash == 3)) 

dd2_new <- dd2_new %>%
  mutate(path = paste(user_repo,"/commits?per_page=1",sep=""))%>%
  rename(package = Package)

pkg_url_new <- dd2_new %>%
  select(package,URL)

#save the clean-up pkg and github URL to scrape data in python
##write_csv(pkg_url_new, file=here::here("data/pkg_url_new.csv"))
#save(dd2_new, file=here::here("data/dd2_new.rda")) 
```

```{r userrepo-tbl}
load("data/dd2_new.rda")

#display some packages with their clean URL as exaple
dd2_new %>%
  select(package,URL,user_repo)%>%
  head(5) %>%
  kable(caption = "First 5 R-packages with their 'user_repo' element") %>%
  kable_styling(bootstrap_options = c("hover", "striped")) 
```


```{r combine-commits,eval=FALSE}
#read data of commits
##the code used to scrape the commits is in python script 'spider.py'
commits_all <- read.table("data/commits.txt", comment="", header=TRUE)
colnames(commits_all)[1] <- "URL"
colnames(commits_all)[2] <- "commits"

pkg_commits_all <- commits_all %>%
  left_join(pkg_url_new,by = "URL") 

pkg_commits_all <- pkg_commits_all %>%
  select(package,commits)

pkg_commits_all <- pkg_commits_all %>%
  left_join(cran_names_download, by = "package")

#save(pkg_commits_all, file=here::here("data/pkg_commits_all.rda"))

```

```{r load-commits}
load("data/pkg_commits_all.rda")
```

Table \@ref(tab:commits-tbl) shows the first 5 R-packages of all, along with their last half a year's total downloads and the number of commits on Github master branch.

```{r commits-tbl}
pkg_commits_all %>%
  arrange(desc(total)) %>%
  head(5) %>%
  kable(caption = "First 5 R-packages with commits on Github and last half year's total downloads") %>%
  kable_styling(bootstrap_options = c("hover", "striped")) 
```

Figure \@ref(fig:commits-pattern) shows the scatterplot along with a smoothing line. In general, more commits, more downloads. 

(ref:commits-pattern) The commits on master branch of Github repository against the last half a year's total download count.

```{r commits-pattern, fig.cap="(ref:commits-pattern)"}
pkg_commits_all %>%
  mutate(total = as.numeric(total)) %>%
  mutate(commits = as.numeric(commits)) %>%
  ggplot(aes(x = commits, y = total)) +
  geom_point() +
  geom_smooth(se = F) +
  scale_y_log10() +
  labs( y = 'Total download count',
          x = 'The number of commits',
    title = "Commits counts against download counts",
    subtitle = "of 5769 R-packages for last half a year") +
    #annotate("text",y= 70000,x= 500,label="In general, more commits, more downloads.",color="red") +
  #theme_minimal()  +
  theme(panel.grid.major = element_blank()) +
  theme(
  axis.text=element_text(size=10),
  axis.title=element_text(size=12,face="bold"),
  plot.title = element_text(h = 0.5),
  plot.subtitle = element_text(h = 0.5))

```

Another method to explore this question is by looking at the ultra-low-downloaded R-packages, whose download count only ranks at last 1% of all. And another purpose for this part is to show how we initially intended to scrape commits through Github API with R (as the sample size is less than 60 R-packages under this situation). 

From Table \@ref(tab:quantileall) we could see the last 1% downloaded count is around 401. And as these download counts are extremely low, we could assume that many factors will have little effect on their downloads. Thus, we can further assume that the only two differences between them are the number of commits on Github master branch and the total download count.

So, we could filter these ultra-low-downloaded R-packages and extract the commits on Github, then explore the pattern in total download count against the number of commits in Figure \@ref(fig:pkg-low). It could be believed that when the number of commits increases from 0 to 100, the download count first increases and then decreases. After that, the download volume keeps rising and appears to have a little jump at the end, but we supposed it might be due to the the too small-sized sample, and the observation causing the decline may be an outlier. So, if expanding the observation horizon, we expected the curve might increase again. Therefore, it can be seen that even for these last 1% downloaded R-packages, there also exist the phenomenon - "the more commits, the more downloads".

```{r quantileall}
quantile_all <- quantile(cran_names_download$total,probs = seq(0, 1, 0.01)) %>% 
  as.data.frame()

colnames(quantile_all)[1] <- "download count"

quantile_all %>%
  kable(caption = "Quantile of total download count for all the R-packages on CRAN") %>%
  kable_styling(full_width = FALSE) %>%
  scroll_box(width = "100%", height = "400px")
```

(ref:pkg-low) The commits on master branch of Github repository against the last half a year's total download count for last 1% downloaded R-packages on CRAN.

```{r pkg-low,fig.cap="(ref:pkg-low)"}
pkg_low_download <- left_join(dd2_new,cran_names_download,by = "package") %>%
  arrange(desc(total)) %>%
  filter(total < 401) 

github_api_low <- map(pkg_low_download$user_repo,function(user_repo) {
  
   #Sys.sleep(60)
  
  resp <- httr::GET(glue::glue("https://api.github.com/repos/{user_repo}/commits?per_page=1"))
  
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }

c <- str_split(headers(resp)$link, ",") %>%
  unlist()


d <- c[str_detect(c,'rel=\"last\"')] 


f <- str_sub(d, str_locate(d, '&page=')[2]+1, str_locate(d, '>')[1]-1)

  return(f)
  
})

#convert the list output to dataframe
commits <- data.frame(matrix(unlist(github_api_low), nrow=length(github_api_low), byrow=TRUE))
colnames(commits)[1] <- "commits"

#append the commits to the original data
pkg_commits_low <-  cbind(pkg_low_download,commits) %>%
  rename(package = package)%>%
  mutate(commits = as.numeric(commits))

pkg_commits_low  %>%
  mutate(total = as.numeric(total)) %>%
  mutate(commits = as.numeric(commits)) %>%
  ggplot(aes(x = commits, y = total)) +
  geom_point() +
  geom_smooth(se = F) +
  labs( y = 'Total download count',
          x = 'The number of commits',
    title = "Commits counts against download counts",
    subtitle = "of last 1% downloaded R-packages for last half year") +
    #annotate("text",y= 70000,x= 500,label="In general, more commits, more downloads.",color="red") +
  #theme_minimal()  +
  theme(panel.grid.major = element_blank()) +
  theme(
  axis.text=element_text(size=10),
  axis.title=element_text(size=12,face="bold"),
  plot.title = element_text(h = 0.5),
  plot.subtitle = element_text(h = 0.5))
```

