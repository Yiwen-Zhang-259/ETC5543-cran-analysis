## Compare download counts with the number of commits on master branch

In this section, we compared total downloads over the most recent 6 month period, with the number of commits on master branch in Github repositories. We initially planned to replace the research object with all of R-packages on CRAN, `r comma(length(pkgs$package),digits =0)` up to now. But we soon found that there were only 6185 R-packages that have Github repositories, and after cleaning up, only 5769 remained.

```{block, type="discovery", echo = TRUE}
**Finding**: In our initial assumption, more commits on master branch of Github repositories are likely to bring R-packages more download counts. On the one hand, the number of commits can indicate that developers are constantly supplementing and updating their R-packages, which may lead to more downloads. On the other hand, the number of commits also can reflect the attention developers attach to their R-packages, to some extent. Usually, for those who have more commits, their developers may advertise more or try other ways, to expand the popularity and improve the download counts.
```

The way we initially planned to apply was extracting the commits through accessing the Github REST API[@githubapi]. To achieve that, we first scraped the Github URLs from 'description' pages for all of R-packages from CRAN, then cleaned up the multiple URLs and other redundant symbols or characters. The URL is formatted as : *"https://api.github.com/repos/{user_repo}/commits?per_page=1"*. For example, the URL for package `tidyverse` can be like : *"https://api.github.com/repos/tidyverse/tidyverse/commits?per_page=1"*. Practically, by replacing the `user_repo` part that consists of the name of the related repository and the name of the package holder, we could access the contents of URLs through Github API, for all of CRAN R-packages.

```{r userrepo,eval=FALSE}
#code used to extract Github URLs for CRAN R-packages

#scrape the Github URLs
getPackageRDS <- function() {
     description <- sprintf("%s/web/packages/packages.rds",
                            getOption("repos")["CRAN"])
     con <- if(substring(description, 1L, 7L) == "file://") {
         file(description, "rb")
     } else {
         url(description, "rb")
     }
     on.exit(close(con))
     db <- readRDS(gzcon(con))
     rownames(db) <- NULL
     return(db)
}

dd <- as.data.frame(getPackageRDS())
dd2 <- subset(dd, grepl("github.com", URL))

## clean up (multiple URLs, etc.)
dd2$URL <- sapply(strsplit(dd2$URL,"[, \n]"),
       function(x) trimws(grep("github.com", x, value=TRUE)[1]))

dd2$URL <- sapply(strsplit(dd2$URL,"[>]"),
       function(x) trimws(grep("github.com", x, value=TRUE)[1]))

dd2$URL <- sapply(strsplit(dd2$URL,"[<]"),
       function(x) trimws(grep("github.com", x, value=TRUE)[1]))

dd2$URL <- sapply(strsplit(dd2$URL,"[#]"),
       function(x) trimws(grep("github.com", x, value=TRUE)[1]))

dd2$URL <- str_replace_all(dd2$URL, "https", "http")


dd2_new <- dd2 %>%
  select(Package,URL) %>%
  mutate(user_repo = str_sub(URL, str_locate(URL, 'github.com/')[2]+11)) 

#filter some obs
a <- dd2_new$user_repo[str_detect(dd2_new$user_repo,'/blob/')]
b <- dd2_new$user_repo[str_detect(dd2_new$user_repo,'/master')]
c <- dd2_new$user_repo[str_detect(dd2_new$user_repo,'/Master')]

#get all the user names and repo names done
dd2_new <- dd2_new %>%
  filter(!(user_repo%in%a))%>%
  filter(!(user_repo%in%b))%>%
  filter(!(user_repo%in%c))

#want to remove the "/" at the last position of user_repo for some pkgs
#count the frequency of "/"
dd2_new$freq_dash <- str_count(dd2_new$user_repo, "/")

#remove the last "/" for those have two "/" in user_repo (and don't have other things inside)
dd2_new_dash <- dd2_new %>%
  filter(freq_dash == 2) %>%
  mutate(user_repo = substr(user_repo,1,nchar(user_repo)-1))
         
#bind with original data and filter some pkgs with unclean user_repo (hard to clean and not many)
dd2_new <- dd2_new %>%
  filter(!(Package %in% dd2_new_dash$Package)) %>%
  rbind(dd2_new_dash) %>%
  arrange(Package)%>%
  filter(!(freq_dash == 0))%>%
  filter(!(freq_dash == 2))%>%
  filter(!(freq_dash == 3)) 

dd2_new <- dd2_new %>%
  mutate(path = paste(user_repo,"/commits?per_page=1",sep=""))%>%
  rename(package = Package)

pkg_url_new <- dd2_new %>%
  select(package,URL)

#save the clean-up pkgs and github URLs to scrape data by python
##write_csv(pkg_url_new, file=here::here("data/pkg_url_new.csv"))
#save(dd2_new, file=here::here("data/dd2_new.rda")) 
```


Based on that, Table \@ref(tab:userrepo-tbl) shows the first 5 R-packages, with their Github URLs and the `user_repo` parts.

However, the Github API has a rate limits, allowing for up to 60 requests per hour for unauthenticated requests, which can be extended to 5000 per hour after authentication[@githubapi]. But even after getting authentication, our rate limit didn't get promoted. So, we switched to scrape commits with Python spider, by setting random user agent, to avoid the API limit. Since this method is quite time consuming (around 5 hours each time for all available R-packages), we saved the output as a local .txt file and load it later in R studio, to speed up the code execution. 

```{r userrepo-tbl}
load("data/dd2_new.rda")

#display some packages with their clean URL as exaple
dd2_new %>%
  select(package,URL,user_repo)%>%
  head(5) %>%
  kable(caption = "First 5 R-packages with their 'user_repo' element") %>%
  kable_styling(bootstrap_options = c("hover", "striped")) 
```


```{r combine-commits}
#read data of commits
##the code used to scrape the commits is in python script 'spider.py'
commits_all <- read.table("data/commits.txt", comment="", header=TRUE)
colnames(commits_all)[1] <- "URL"
colnames(commits_all)[2] <- "commits"

pkg_commits_all <- commits_all %>%
  left_join(pkg_url_new,by = "URL") 

pkg_commits_all <- pkg_commits_all %>%
  select(package,commits)

pkg_commits_all <- pkg_commits_all %>%
  left_join(cran_names_download, by = "package")

```

Table \@ref(tab:commits-tbl) shows the first 5 R-packages of all, along with their total downloads over the most recent 6 month period, and the numbers of commits on master branch in Github repositories.

```{r commits-tbl}
pkg_commits_all %>%
  arrange(desc(total)) %>%
  head(5) %>%
  kable(caption = "R-packages with commits on Github and total downloads over the most recent 6 month") %>%
  kable_styling(bootstrap_options = c("hover", "striped")) 
```

Figure \@ref(fig:commits-pattern) shows the scatterplot, along with a smoothing line. In general, more commits can link to more downloads. 

(ref:commits-pattern) The commits on master branch of Github repositories against the total download counts over the most recent 6 month period.

```{r commits-pattern, fig.cap="(ref:commits-pattern)"}
pkg_commits_all %>%
  mutate(total = as.numeric(total)) %>%
  mutate(commits = as.numeric(commits)) %>%
  ggplot(aes(x = commits, y = total)) +
  geom_point() +
  geom_smooth(se = F) +
  scale_y_log10() +
  labs( y = 'Total download count',
          x = 'The number of commits',
    title = "Commits counts against download counts",
    subtitle = "of 5769 R-packages over the most recent 6 month") +
    #annotate("text",y= 70000,x= 500,label="In general, more commits, more downloads.",color="red") +
  #theme_minimal()  +
  theme(panel.grid.major = element_blank()) +
  theme(
  axis.text=element_text(size=10),
  axis.title=element_text(size=12,face="bold"),
  plot.title = element_text(h = 0.5),
  plot.subtitle = element_text(h = 0.5))

```

Another method can be conducted by taking the ultra-low-downloaded R-packages into consideration, whose download counts only rank last 1% of all. Another purpose for this way is to show our initial idea on scraping commits through Github API with R (as the sample size is less than 60 R-packages, on this condition). 

From Table \@ref(tab:quantileall), it can be seen that the last 1% download count is around 401. And as the download counts of those packages are extremely low, we could assume that many factors can have little effect on their downloads, and the only two differences among them are the numbers of commits on Github master branch and the total download counts.

Based above, we could select those ultra-low-downloaded R-packages and extract commit counts on their Github repositories. In Figure \@ref(fig:pkg-low), it could be observed that when the number of commits increases from 0 to 100, the download counts first increase and then decrease. After that, the download volume keeps rising, with a small jump at the end. We consider that might result from the too small-sized sample, and the observation causing the decline can be an outlier. If expanding the observation horizon, an increase in this curve can be expected. Therefore, it can be concluded that even for those last 1% downloaded R-packages, the phenomenon - "the more commits can link to  more downloads" also exists.

```{r quantileall}
quantile_all <- quantile(cran_names_download$total,probs = seq(0, 1, 0.01)) %>% 
  as.data.frame()

colnames(quantile_all)[1] <- "download count"

quantile_all %>%
  kable(caption = "Quantile of total download counts for all of R-packages on CRAN") %>%
  kable_styling(full_width = FALSE) %>%
  scroll_box(width = "100%", height = "400px")
```

(ref:pkg-low) The commits on master branch of Github repositories, against the total download counts over the most recent 6 month period, for last 1% downloaded R-packages on CRAN.

```{r pkg-low,fig.cap="(ref:pkg-low)"}
pkg_low_download <- left_join(dd2_new,cran_names_download,by = "package") %>%
  arrange(desc(total)) %>%
  filter(total < 401) 

github_api_low <- map(pkg_low_download$user_repo,function(user_repo) {
  
   #Sys.sleep(60)
  
  resp <- httr::GET(glue::glue("https://api.github.com/repos/{user_repo}/commits?per_page=1"))
  
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }

c <- str_split(headers(resp)$link, ",") %>%
  unlist()


d <- c[str_detect(c,'rel=\"last\"')] 


f <- str_sub(d, str_locate(d, '&page=')[2]+1, str_locate(d, '>')[1]-1)

  return(f)
  
})

#convert the list output to dataframe
commits <- data.frame(matrix(unlist(github_api_low), nrow=length(github_api_low), byrow=TRUE))
colnames(commits)[1] <- "commits"

#append the commits to the original data
pkg_commits_low <-  cbind(pkg_low_download,commits) %>%
  rename(package = package)%>%
  mutate(commits = as.numeric(commits))

pkg_commits_low  %>%
  mutate(total = as.numeric(total)) %>%
  mutate(commits = as.numeric(commits)) %>%
  ggplot(aes(x = commits, y = total)) +
  geom_point() +
  geom_smooth(se = F) +
  labs( y = 'Total download count',
          x = 'The number of commits',
    title = "Commit counts against download counts",
    subtitle = "for last 1% downloaded CRAN R-packages over the most recent 6 month") +
    #annotate("text",y= 70000,x= 500,label="In general, more commits, more downloads.",color="red") +
  #theme_minimal()  +
  theme(panel.grid.major = element_blank()) +
  theme(
  axis.text=element_text(size=10),
  axis.title=element_text(size=12,face="bold"),
  plot.title = element_text(h = 0.5),
  plot.subtitle = element_text(h = 0.5))
```

